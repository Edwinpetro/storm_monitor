{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from datetime import datetime, timezone\n",
    "import re\n",
    "\n",
    "def extract_storm_info(dat_file_path):\n",
    "    \"\"\"\n",
    "    Extract storm name, start date, and end date from the .dat file's content.\n",
    "    \"\"\"\n",
    "    storm_name = None\n",
    "    storm_start_date = None\n",
    "    storm_end_date = None\n",
    "    dates = []\n",
    "    storm_name_dict = {}\n",
    "\n",
    "    # List of known basin codes\n",
    "    basin_codes = [\"AL\", \"EP\", \"CP\", \"WP\", \"IO\", \"SH\"]\n",
    "\n",
    "    # Set of number words up to 20\n",
    "    number_words_set = {\n",
    "        'ONE', 'TWO', 'THREE', 'FOUR', 'FIVE', 'SIX', 'SEVEN', 'EIGHT', 'NINE', 'TEN',\n",
    "        'ELEVEN', 'TWELVE', 'THIRTEEN', 'FOURTEEN', 'FIFTEEN', 'SIXTEEN', 'SEVENTEEN',\n",
    "        'EIGHTEEN', 'NINETEEN', 'TWENTY'\n",
    "    }\n",
    "\n",
    "    with open(dat_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            columns = line.strip().split()\n",
    "\n",
    "            if len(columns) >= 28:\n",
    "                potential_name = columns[27].strip().rstrip(',').upper()\n",
    "\n",
    "                # Add potential_name to the dictionary\n",
    "                if potential_name:\n",
    "                    storm_name_dict[potential_name] = storm_name_dict.get(potential_name, 0) + 1\n",
    "\n",
    "            # Parse dates as before\n",
    "            if len(columns) >= 3:\n",
    "                date_str = columns[2].strip()\n",
    "                date_str = re.sub(r'[^0-9]', '', date_str)\n",
    "                if len(date_str) == 10:\n",
    "                    try:\n",
    "                        date = datetime.strptime(date_str, \"%Y%m%d%H\")\n",
    "                        dates.append(date)\n",
    "                    except ValueError:\n",
    "                        print(f\"Error parsing date: {date_str} in file {dat_file_path}\")\n",
    "\n",
    "    # Step 1: Remove basin codes from the dictionary keys\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if k not in basin_codes}\n",
    "\n",
    "    # Step 2: Remove entries that contain digits\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if not any(char.isdigit() for char in k)}\n",
    "\n",
    "    # Step 3: Remove entries that are in number_words\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if k not in number_words_set}\n",
    "\n",
    "    # Get list of storm names\n",
    "    storm_names = list(storm_name_dict.keys())\n",
    "\n",
    "    # Determine the storm name based on the cleaned list\n",
    "    if len(storm_names) == 0:\n",
    "        storm_name = 'DISTURBANCE'\n",
    "    elif storm_names == ['INVEST']:\n",
    "        storm_name = 'INVEST'\n",
    "    else:\n",
    "        # Remove 'INVEST' if other names are present\n",
    "        if 'INVEST' in storm_names:\n",
    "            storm_names.remove('INVEST')\n",
    "\n",
    "        if len(storm_names) == 1:\n",
    "            storm_name = storm_names[0]\n",
    "        else:\n",
    "            # Multiple names remain\n",
    "            # Attempt to strip basin codes from ends\n",
    "            cleaned_names = set()\n",
    "            for name in storm_names:\n",
    "                cleaned_name = name\n",
    "                for basin_code in basin_codes:\n",
    "                    if name.endswith(basin_code):\n",
    "                        name_without_basin = name[:-len(basin_code)].strip()\n",
    "                        # Ensure the name is non-empty and alphabetical\n",
    "                        if name_without_basin.isalpha():\n",
    "                            cleaned_name = name_without_basin\n",
    "                            break  # Stop checking after removing basin code\n",
    "                cleaned_names.add(cleaned_name)\n",
    "\n",
    "            if len(cleaned_names) == 1:\n",
    "                storm_name = cleaned_names.pop()\n",
    "            else:\n",
    "                # Names don't match, take the last one\n",
    "                storm_name = storm_names[-1]\n",
    "\n",
    "  # Deduce the start and end dates and extract the year\n",
    "    if dates:\n",
    "        storm_start_datetime = min(dates)\n",
    "        storm_end_datetime = max(dates)\n",
    "        storm_start_date = storm_start_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        storm_end_date = storm_end_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        storm_year = storm_end_datetime.year  # Extract the year\n",
    "    else:\n",
    "        storm_start_date = None\n",
    "        storm_end_date = None\n",
    "        storm_year = None\n",
    "\n",
    "    return storm_name.title(), storm_start_date, storm_end_date, storm_year\n",
    "\n",
    "def process_dat_files(input_path, output_csv):\n",
    "    \"\"\"\n",
    "    Process all .dat files in the input path, extract required information, \n",
    "    and save it in a .csv file.\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Loop over all files in the input path\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.dat'):\n",
    "            # Extract Basin and Storm Number from the file name\n",
    "            basin = file_name[1:3]  # Two letters after \"a\"\n",
    "            storm_number = file_name[3:5]  # Two numbers after basin\n",
    "            \n",
    "            # Full path to the .dat file\n",
    "            dat_file_path = os.path.join(input_path, file_name)\n",
    "            \n",
    "            # Extract Storm Name, Start Date, and End Date from the .dat file\n",
    "            storm_name, storm_start_date, storm_end_date, storm_year = extract_storm_info(dat_file_path)\n",
    "            \n",
    "            # Append the extracted information as a row\n",
    "            data_rows.append([basin, storm_number, storm_name, storm_start_date, storm_end_date, storm_year, dat_file_path])\n",
    "    \n",
    "    # Write the collected data to a CSV file\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Basin', 'Storm_Number', 'Storm_Name', 'Storm_Start_Date', 'Storm_End_Date','Storm_Year','adeck_path'])\n",
    "        # Write the data rows\n",
    "        writer.writerows(data_rows)\n",
    "\n",
    "    print(f\"CSV file {output_csv} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file storm_adeck_directory.csv has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_directory = \"./forecast_data/\"  # Replace with the actual path to the .dat files\n",
    "output_csv_file = \"storm_adeck_directory.csv\"\n",
    "process_dat_files(input_directory, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(2024, 2025)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_year = datetime.now().year\n",
    "current_year = datetime.now().year\n",
    "range(start_year, current_year+1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EventMonitor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
