{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def extract_storm_info(dat_file_path):\n",
    "    \"\"\"\n",
    "    Extract storm name, start date, and end date from the .dat file's content.\n",
    "    - The third column has the date in the format YYYYMMDDHH.\n",
    "    - The storm name is in the 28th column.\n",
    "    - The storm name returned is the last one in the dictionary of names.\n",
    "    - If no name is found, label as \"INVEST\" or \"DISTURBANCE\".\n",
    "    \"\"\"\n",
    "    storm_name = None\n",
    "    storm_start_date = None\n",
    "    storm_end_date = None\n",
    "    dates = []\n",
    "    storm_name_dict = {}  # Dictionary to hold all unique values from column 28\n",
    "\n",
    "    with open(dat_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            columns = line.split()\n",
    "\n",
    "            # Process the 28th column to capture storm names (if the file has enough columns)\n",
    "            if len(columns) >= 28:\n",
    "                potential_name = columns[27].strip().rstrip(',')  # Remove trailing commas\n",
    "                # Add potential_name to the dictionary, count occurrences\n",
    "                if potential_name:\n",
    "                    storm_name_dict[potential_name] = storm_name_dict.get(potential_name, 0) + 1\n",
    "\n",
    "            # Parse the date from the third column (format: YYYYMMDDHH)\n",
    "            if len(columns) >= 3:\n",
    "                date_str = columns[2].strip()\n",
    "                date_str = re.sub(r'[^0-9]', '', date_str)\n",
    "\n",
    "                if len(date_str) == 10:  # Ensure it's the correct length (YYYYMMDDHH)\n",
    "                    try:\n",
    "                        # Convert the string to a datetime object using the correct format\n",
    "                        date = datetime.datetime.strptime(date_str, \"%Y%m%d%H\")\n",
    "                        dates.append(date)\n",
    "                    except ValueError:\n",
    "                        print(f\"Error parsing date: {date_str} in file {dat_file_path}\")\n",
    "\n",
    "        # Apply logic to determine the storm name from the dictionary\n",
    "        print(f\"Storm Name Dictionary: {storm_name_dict}\")  # Debugging output\n",
    "\n",
    "        # Remove irrelevant entries such as empty commas\n",
    "        storm_name_dict = {k: v for k, v in storm_name_dict.items() if k not in [',']}\n",
    "\n",
    "        # Determine the last storm name in the dictionary\n",
    "        if storm_name_dict:\n",
    "            storm_name = list(storm_name_dict.keys())[-1]  # Get the last key (storm name) in the dictionary\n",
    "        else:\n",
    "            storm_name = \"DISTURBANCE\"  # If no valid names, return DISTURBANCE\n",
    "\n",
    "        # Deduce the start and end dates\n",
    "        if dates:\n",
    "            storm_start_date = min(dates).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "            storm_end_date = max(dates).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return storm_name, storm_start_date, storm_end_date\n",
    "\n",
    "def process_dat_files(input_path, output_csv):\n",
    "    \"\"\"\n",
    "    Process all .dat files in the input path, extract required information, \n",
    "    and save it in a .csv file.\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "    \n",
    "    # Loop over all files in the input path\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.dat'):\n",
    "            # Extract Basin and Storm Number from the file name\n",
    "            basin = file_name[1:3]  # Two letters after \"a\"\n",
    "            storm_number = file_name[3:5]  # Two numbers after basin\n",
    "            \n",
    "            # Full path to the .dat file\n",
    "            dat_file_path = os.path.join(input_path, file_name)\n",
    "            \n",
    "            # Extract Storm Name, Start Date, and End Date from the .dat file\n",
    "            storm_name, storm_start_date, storm_end_date = extract_storm_info(dat_file_path)\n",
    "            \n",
    "            # Append the extracted information as a row\n",
    "            data_rows.append([basin, storm_number, storm_name, storm_start_date, storm_end_date])\n",
    "    \n",
    "    # Write the collected data to a CSV file\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Basin', 'Storm Number', 'Storm Name', 'Storm Start Date', 'Storm End Date'])\n",
    "        # Write the data rows\n",
    "        writer.writerows(data_rows)\n",
    "\n",
    "    print(f\"CSV file {output_csv} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Storm Name Dictionary: {'INVEST': 12, 'ONE': 6, 'ARLENE': 33}\n",
      "Storm Name Dictionary: {'INVEST': 21, 'ONE': 12, 'ALBERTO': 39}\n",
      "Storm Name Dictionary: {'INVEST': 12, 'ANDREA': 42}\n",
      "Storm Name Dictionary: {'INVEST': 6, 'ARTHUR': 18}\n",
      "Storm Name Dictionary: {'INVEST': 24, 'ONE': 15}\n",
      "Storm Name Dictionary: {'INVEST': 54, 'ONE': 3, 'ALEX': 72}\n",
      "Storm Name Dictionary: {'INVEST': 28, 'ARLENE': 70}\n",
      "Storm Name Dictionary: {'INVEST': 14, 'ALBERTO': 77}\n",
      "Storm Name Dictionary: {'INVEST': 56, 'ANDREA': 84}\n",
      "Storm Name Dictionary: {'INVEST': 70, 'ONE': 21, 'ARTHUR': 112}\n",
      "Storm Name Dictionary: {}\n",
      "Storm Name Dictionary: {'INVEST': 21, 'FOUR': 15, 'DEAN': 102}\n",
      "Storm Name Dictionary: {'INVEST': 87, 'DOLLY': 54}\n",
      "Storm Name Dictionary: {'INVEST': 3, 'FOUR': 6, 'CLAUDETTE': 9}\n",
      "Storm Name Dictionary: {'INVEST': 18, 'FOUR': 9, 'COLIN': 66}\n",
      "Storm Name Dictionary: {'INVEST': 91, 'FOUR': 7, 'DON': 70}\n",
      "Storm Name Dictionary: {'INVEST': 35, 'FIVE': 14, 'ERIN': 98}\n",
      "Storm Name Dictionary: {}\n",
      "Storm Name Dictionary: {'INVEST': 63, 'ERIKA': 154}\n",
      "Storm Name Dictionary: {'INVEST': 126, 'EARL': 119}\n",
      "Storm Name Dictionary: {'INVEST': 42, 'DON': 42}\n",
      "Storm Name Dictionary: {'INVEST': 56, 'FIVE': 7, 'ERNESTO': 77}\n",
      "Storm Name Dictionary: {'INVEST': 35, 'FIVE': 14, 'DORIAN': 427}\n",
      "Storm Name Dictionary: {'INVEST': 14, 'FIVE': 49, 'EDOUARD': 21}\n",
      "Storm Name Dictionary: {'INVEST': 42, 'FIVE': 7, '05L': 30, 'ELSA': 465}\n",
      "CSV file storm_adeck_directory.csv has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_directory = \"./test_forecast_data/\"  # Replace with the actual path to the .dat files\n",
    "output_csv_file = \"storm_adeck_directory.csv\"\n",
    "process_dat_files(input_directory, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "def extract_storm_info_by_carq(dat_file_path):\n",
    "    \"\"\"\n",
    "    Extract storm name, start date, and end date from the .dat file's content, prioritizing rows where column 5 == \"CARQ\"\n",
    "    and picking the storm name from the row closest to the storm's end date.\n",
    "    \"\"\"\n",
    "    storm_name = None\n",
    "    storm_start_date = None\n",
    "    storm_end_date = None\n",
    "    dates = []\n",
    "    carq_rows = []\n",
    "\n",
    "    with open(dat_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            columns = line.split()\n",
    "\n",
    "            # Parse the date from the third column (format: YYYYMMDDHH)\n",
    "            if len(columns) >= 3:\n",
    "                date_str = columns[2].strip()\n",
    "                date_str = re.sub(r'[^0-9]', '', date_str)\n",
    "\n",
    "                if len(date_str) == 10:  # Ensure it's the correct length (YYYYMMDDHH)\n",
    "                    try:\n",
    "                        date = datetime.datetime.strptime(date_str, \"%Y%m%d%H\")\n",
    "                        dates.append(date)\n",
    "\n",
    "                        # Check if the 5th column == \"CARQ\"\n",
    "                        if len(columns) >= 28 and columns[4].strip() == \"CARQ\":\n",
    "                            # Store the row's date, the 28th column (storm name), and the row's date\n",
    "                            carq_rows.append((date, columns[27].strip().rstrip(',')))\n",
    "\n",
    "                    except ValueError:\n",
    "                        print(f\"Error parsing date: {date_str} in file {dat_file_path}\")\n",
    "\n",
    "        # If there are no valid dates or CARQ rows, return \"DISTURBANCE\"\n",
    "        if not dates or not carq_rows:\n",
    "            return \"DISTURBANCE\", None, None\n",
    "\n",
    "        # Deduce the start and end dates\n",
    "        storm_start_date = min(dates).strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        storm_end_date = max(dates)\n",
    "\n",
    "        # Find the row where CARQ is closest to the end date\n",
    "        closest_carq_row = min(carq_rows, key=lambda x: abs(x[0] - storm_end_date))\n",
    "\n",
    "        # The storm name is the 28th column in the closest CARQ row\n",
    "        storm_name = closest_carq_row[1]\n",
    "\n",
    "        # Convert the end date to string format\n",
    "        storm_end_date = storm_end_date.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "    return storm_name, storm_start_date, storm_end_date\n",
    "\n",
    "def process_dat_files(input_path, output_csv):\n",
    "    \"\"\"\n",
    "    Process all .dat files in the input path, extract required information, \n",
    "    and save it in a .csv file.\n",
    "    \"\"\"\n",
    "    data_rows = []\n",
    "\n",
    "    # Loop over all files in the input path\n",
    "    for file_name in os.listdir(input_path):\n",
    "        if file_name.endswith('.dat'):\n",
    "            # Extract Basin and Storm Number from the file name\n",
    "            basin = file_name[1:3]  # Two letters after \"a\"\n",
    "            storm_number = file_name[3:5]  # Two numbers after basin\n",
    "\n",
    "            # Full path to the .dat file\n",
    "            dat_file_path = os.path.join(input_path, file_name)\n",
    "\n",
    "            # Extract Storm Name, Start Date, and End Date from the .dat file\n",
    "            storm_name, storm_start_date, storm_end_date = extract_storm_info_by_carq(dat_file_path)\n",
    "\n",
    "            # Append the extracted information as a row\n",
    "            data_rows.append([basin, storm_number, storm_name, storm_start_date, storm_end_date])\n",
    "\n",
    "    # Write the collected data to a CSV file\n",
    "    with open(output_csv, 'w', newline='') as csv_file:\n",
    "        writer = csv.writer(csv_file)\n",
    "        # Write the header\n",
    "        writer.writerow(['Basin', 'Storm Number', 'Storm Name', 'Storm Start Date', 'Storm End Date'])\n",
    "        # Write the data rows\n",
    "        writer.writerows(data_rows)\n",
    "\n",
    "    print(f\"CSV file {output_csv} has been created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV file storm_adeck_directory_carq.csv has been created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "input_directory = \"./test_forecast_data/\"  # Replace with the actual path to the .dat files\n",
    "output_csv_file = \"storm_adeck_directory_carq.csv\"\n",
    "process_dat_files(input_directory, output_csv_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def extract_storm_info(dat_file_path):\n",
    "    \"\"\"\n",
    "    Extract storm name, start date, and end date from the .dat file's content.\n",
    "    \"\"\"\n",
    "    storm_name = None\n",
    "    storm_start_date = None\n",
    "    storm_end_date = None\n",
    "    dates = []\n",
    "    storm_name_dict = {}\n",
    "\n",
    "    # List of known basin codes\n",
    "    basin_codes = [\"AL\", \"EP\", \"CP\", \"WP\", \"IO\", \"SH\"]\n",
    "\n",
    "    # Set of number words up to 20\n",
    "    number_words_set = {\n",
    "        'ONE', 'TWO', 'THREE', 'FOUR', 'FIVE', 'SIX', 'SEVEN', 'EIGHT', 'NINE', 'TEN',\n",
    "        'ELEVEN', 'TWELVE', 'THIRTEEN', 'FOURTEEN', 'FIFTEEN', 'SIXTEEN', 'SEVENTEEN',\n",
    "        'EIGHTEEN', 'NINETEEN', 'TWENTY'\n",
    "    }\n",
    "\n",
    "    with open(dat_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            columns = line.strip().split()\n",
    "\n",
    "            if len(columns) >= 28:\n",
    "                potential_name = columns[27].strip().rstrip(',').upper()\n",
    "\n",
    "                # Add potential_name to the dictionary\n",
    "                if potential_name:\n",
    "                    storm_name_dict[potential_name] = storm_name_dict.get(potential_name, 0) + 1\n",
    "\n",
    "            # Parse dates as before\n",
    "            if len(columns) >= 3:\n",
    "                date_str = columns[2].strip()\n",
    "                date_str = re.sub(r'[^0-9]', '', date_str)\n",
    "                if len(date_str) == 10:\n",
    "                    try:\n",
    "                        date = datetime.strptime(date_str, \"%Y%m%d%H\")\n",
    "                        dates.append(date)\n",
    "                    except ValueError:\n",
    "                        print(f\"Error parsing date: {date_str} in file {dat_file_path}\")\n",
    "\n",
    "    # Step 1: Remove basin codes from the dictionary keys\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if k not in basin_codes}\n",
    "\n",
    "    # Step 2: Remove entries that contain digits\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if not any(char.isdigit() for char in k)}\n",
    "\n",
    "    # Step 3: Remove entries that are in number_words\n",
    "    storm_name_dict = {k: v for k, v in storm_name_dict.items() if k not in number_words_set}\n",
    "\n",
    "    # Get list of storm names\n",
    "    storm_names = list(storm_name_dict.keys())\n",
    "\n",
    "    # Determine the storm name based on the cleaned list\n",
    "    if len(storm_names) == 0:\n",
    "        storm_name = 'DISTURBANCE'\n",
    "    elif storm_names == ['INVEST']:\n",
    "        storm_name = 'INVEST'\n",
    "    else:\n",
    "        # Remove 'INVEST' if other names are present\n",
    "        if 'INVEST' in storm_names:\n",
    "            storm_names.remove('INVEST')\n",
    "\n",
    "        if len(storm_names) == 1:\n",
    "            storm_name = storm_names[0]\n",
    "        else:\n",
    "            # Multiple names remain\n",
    "            # Attempt to strip basin codes from ends\n",
    "            cleaned_names = set()\n",
    "            for name in storm_names:\n",
    "                cleaned_name = name\n",
    "                for basin_code in basin_codes:\n",
    "                    if name.endswith(basin_code):\n",
    "                        name_without_basin = name[:-len(basin_code)].strip()\n",
    "                        # Ensure the name is non-empty and alphabetical\n",
    "                        if name_without_basin.isalpha():\n",
    "                            cleaned_name = name_without_basin\n",
    "                            break  # Stop checking after removing basin code\n",
    "                cleaned_names.add(cleaned_name)\n",
    "\n",
    "            if len(cleaned_names) == 1:\n",
    "                storm_name = cleaned_names.pop()\n",
    "            else:\n",
    "                # Names don't match, take the last one\n",
    "                storm_name = storm_names[-1]\n",
    "\n",
    "  # Deduce the start and end dates and extract the year\n",
    "    if dates:\n",
    "        storm_start_datetime = min(dates)\n",
    "        storm_end_datetime = max(dates)\n",
    "        storm_start_date = storm_start_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        storm_end_date = storm_end_datetime.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "        storm_year = storm_start_datetime.year  # Extract the year\n",
    "    else:\n",
    "        storm_start_date = None\n",
    "        storm_end_date = None\n",
    "        storm_year = None\n",
    "\n",
    "    return storm_name.title(), storm_start_date, storm_end_date, storm_year\n",
    "\n",
    "def download_and_update_storm_data(file_url, save_path, csv_path):\n",
    "    \"\"\"\n",
    "    Download the .dat file from the given URL, extract storm information,\n",
    "    and update the CSV file with the new or updated storm data.\n",
    "    \"\"\"\n",
    "    # Download the data\n",
    "    download_file(file_url, save_path)\n",
    "    \n",
    "    # Process the .dat file to extract storm info\n",
    "    storm_name, storm_start_date, storm_end_date, storm_year = extract_storm_info(save_path)\n",
    "    \n",
    "    # Extract Basin and Storm Number from the file name\n",
    "    file_name = os.path.basename(save_path)\n",
    "    basin = file_name[1:3]  # Adjust based on actual file naming convention\n",
    "    storm_number = file_name[3:5]  # Adjust based on actual file naming convention\n",
    "\n",
    "    # Read existing CSV or create a new DataFrame\n",
    "    if os.path.exists(csv_path):\n",
    "        df = pd.read_csv(csv_path)\n",
    "    else:\n",
    "        df = pd.DataFrame(columns=['Basin', 'Storm Number', 'Year', 'Storm Name', 'Storm Start Date', 'Storm End Date'])\n",
    "    \n",
    "    # Ensure storm_year is not None\n",
    "    if storm_year is None:\n",
    "        # If storm_year is not available, you may choose to skip this storm or handle it accordingly\n",
    "        print(f\"Storm year not found for {file_name}. Skipping update.\")\n",
    "        return\n",
    "    \n",
    "    # Convert data types for consistency\n",
    "    storm_number = str(storm_number)\n",
    "    basin = str(basin)\n",
    "    storm_year = int(storm_year)\n",
    "\n",
    "    # Check if the storm already exists in the CSV using Basin, Storm Number, and Year\n",
    "    storm_exists = ((df['Basin'] == basin) & (df['Storm Number'] == storm_number) & (df['Year'] == storm_year)).any()\n",
    "    \n",
    "    if storm_exists:\n",
    "        # Update the existing storm information\n",
    "        df.loc[(df['Basin'] == basin) & (df['Storm Number'] == storm_number) & (df['Year'] == storm_year),\n",
    "               ['Storm Name', 'Storm Start Date', 'Storm End Date']] = [storm_name, storm_start_date, storm_end_date]\n",
    "        print(f\"Updated existing storm data for {storm_name} ({basin}{storm_number}, {storm_year}) in {csv_path}.\")\n",
    "    else:\n",
    "        # Append the new storm information\n",
    "        new_row = {\n",
    "            'Basin': basin,\n",
    "            'Storm Number': storm_number,\n",
    "            'Year': storm_year,\n",
    "            'Storm Name': storm_name,\n",
    "            'Storm Start Date': storm_start_date,\n",
    "            'Storm End Date': storm_end_date\n",
    "        }\n",
    "        df = pd.concat([df, pd.DataFrame([new_row])], ignore_index=True)\n",
    "        print(f\"Added new storm data for {storm_name} ({basin}{storm_number}, {storm_year}) to {csv_path}.\")\n",
    "    \n",
    "    # Save the updated CSV\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"Storm data has been updated in {csv_path}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST MAP MAKING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "def create_forecast_map(dat_file_path, selected_forecast_datetime):\n",
    "    \"\"\"\n",
    "    Reads the .dat file and creates a Plotly map of the forecast tracks.\n",
    "\n",
    "    Parameters:\n",
    "    - dat_file_path: str, path to the .dat file.\n",
    "    - selected_forecast_datetime: str, the forecast date and time in YYYYMMDDHH format.\n",
    "\n",
    "    Returns:\n",
    "    - fig: Plotly figure object.\n",
    "    \"\"\"\n",
    "    # Check if the file exists\n",
    "    if not os.path.exists(dat_file_path):\n",
    "        print(f\"File not found: {dat_file_path}\")\n",
    "        return None\n",
    "\n",
    "    # Define column names based on ATCF A-deck format\n",
    "    columns = [\n",
    "        'Basin', 'CycloneNumber', 'DateTime', 'ModelNumber', 'ModelName', 'ForecastHour',\n",
    "        'Latitude', 'Longitude', 'MaxWindSpeed', 'MinPressure',\n",
    "        'WindRad1', 'WindRad2', 'WindRad3', 'WindRad4', 'StormType',\n",
    "        'Quadrant1', 'Quadrant2', 'Quadrant3', 'Quadrant4',\n",
    "        'Radius1', 'Radius2', 'Radius3', 'Radius4',\n",
    "        'StormName', 'Unused1', 'Unused2'\n",
    "    ]\n",
    "    data = []\n",
    "\n",
    "    # Read the .dat file line by line\n",
    "    try:\n",
    "        with open(dat_file_path, 'r') as file:\n",
    "            for line_number, line in enumerate(file, start=1):\n",
    "                # Remove leading/trailing whitespace\n",
    "                line = line.strip()\n",
    "                # Skip empty lines\n",
    "                if not line:\n",
    "                    continue\n",
    "                # Split the line into fields\n",
    "                fields = line.split(',')\n",
    "                # Strip whitespace from each field\n",
    "                fields = [field.strip() for field in fields]\n",
    "                # Handle variable number of fields\n",
    "                # Create a dictionary for this line\n",
    "                record = {}\n",
    "                num_fields = len(fields)\n",
    "                for i in range(min(num_fields, len(columns))):\n",
    "                    record[columns[i]] = fields[i]\n",
    "                # If there are extra fields, add them as 'ExtraField1', 'ExtraField2', etc.\n",
    "                if num_fields > len(columns):\n",
    "                    for j in range(len(columns), num_fields):\n",
    "                        record[f'ExtraField{j - len(columns) + 1}'] = fields[j]\n",
    "                data.append(record)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading .dat file at line {line_number}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Filter data based on selected forecast date and time\n",
    "    df = df[df['DateTime'] == selected_forecast_datetime]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No data found for the selected forecast date and time.\")\n",
    "        return None\n",
    "\n",
    "    # Convert columns to appropriate data types\n",
    "    df['ForecastHour'] = pd.to_numeric(df['ForecastHour'], errors='coerce')\n",
    "    df['MaxWindSpeed'] = pd.to_numeric(df['MaxWindSpeed'], errors='coerce')\n",
    "    df['MaxWindSpeed_mph'] = df['MaxWindSpeed'] * 1.15078  # Knots to miles per hour\n",
    "    df['Latitude'] = df['Latitude'].apply(parse_lat_lon)\n",
    "    df['Longitude'] = df['Longitude'].apply(parse_lat_lon)\n",
    "    df['DateTime'] = pd.to_datetime(df['DateTime'], format='%Y%m%d%H', errors='coerce')\n",
    "\n",
    "    # Drop rows with missing data\n",
    "    before_dropna = len(df)\n",
    "    df.dropna(subset=['Latitude', 'Longitude', 'MaxWindSpeed', 'DateTime', 'ForecastHour'], inplace=True)\n",
    "    after_dropna = len(df)\n",
    "    dropped_na = before_dropna - after_dropna\n",
    "    if dropped_na > 0:\n",
    "        print(f\"Dropped {dropped_na} rows due to missing data.\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"All data has been dropped after removing rows with missing values.\")\n",
    "        return None\n",
    "\n",
    "    # Remove any rows where Latitude or Longitude is exactly 0\n",
    "    before_zero_filter = len(df)\n",
    "    df = df[(df['Latitude'] != 0) & (df['Longitude'] != 0)]\n",
    "    after_zero_filter = len(df)\n",
    "    removed_zero = before_zero_filter - after_zero_filter\n",
    "    if removed_zero > 0:\n",
    "        print(f\"Removed {removed_zero} rows with Latitude or Longitude equal to 0.\")\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"All data has been dropped after removing rows with Latitude or Longitude equal to 0.\")\n",
    "        return None\n",
    "\n",
    "    # Create 'ValidTime' as DateTime + ForecastHour\n",
    "    df['ValidTime'] = df['DateTime'] + pd.to_timedelta(df['ForecastHour'], unit='h')\n",
    "\n",
    "    # Categorize intensity into storm categories\n",
    "    df['Category'] = df['MaxWindSpeed'].apply(wind_speed_to_category)\n",
    "\n",
    "    # Define category colors in the desired order\n",
    "    category_colors = {\n",
    "        'Tropical Depression': 'green',\n",
    "        'Tropical Storm': 'blue',\n",
    "        'Category 1': 'yellow',\n",
    "        'Category 2': 'orange',\n",
    "        'Category 3': 'red',\n",
    "        'Category 4': 'orange',\n",
    "        'Category 5': 'magenta',\n",
    "        'Unknown': 'gray'\n",
    "    }\n",
    "\n",
    "    # Define the desired order for the legend\n",
    "    category_order = [\n",
    "        'Tropical Depression',\n",
    "        'Tropical Storm',\n",
    "        'Category 1',\n",
    "        'Category 2',\n",
    "        'Category 3',\n",
    "        'Category 4',\n",
    "        'Category 5',\n",
    "        'Unknown'\n",
    "    ]\n",
    "\n",
    "    # Map category to colors based on the updated category_colors\n",
    "    df['Color'] = df['Category'].apply(lambda x: category_colors.get(x, 'gray'))\n",
    "\n",
    "    # Sort the DataFrame by ModelName and ValidTime to ensure lines connect correctly per model\n",
    "    df = df.sort_values(['ModelName', 'ValidTime'])\n",
    "\n",
    "    # Initialize the Plotly figure\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add a line trace for each model (white lines connecting points of the same model)\n",
    "    models = df['ModelName'].unique()\n",
    "    for model in models:\n",
    "        model_df = df[df['ModelName'] == model].sort_values('ValidTime')\n",
    "        if model_df.empty:\n",
    "            continue\n",
    "        fig.add_trace(go.Scattermapbox(\n",
    "            lat=model_df['Latitude'],\n",
    "            lon=model_df['Longitude'],\n",
    "            mode='lines',\n",
    "            line=dict(color='white', width=2),\n",
    "            hoverinfo='none',  # No hover info for the lines\n",
    "            showlegend=False    # Do not show lines in the legend\n",
    "        ))\n",
    "\n",
    "    # Group data by Category in the specified order and plot the markers\n",
    "    for category in category_order:\n",
    "        category_df = df[df['Category'] == category]\n",
    "        if not category_df.empty:\n",
    "            fig.add_trace(go.Scattermapbox(\n",
    "                lat=category_df['Latitude'],\n",
    "                lon=category_df['Longitude'],\n",
    "                mode='markers',\n",
    "                name=category,\n",
    "                marker=dict(\n",
    "                    size=8,\n",
    "                    color=category_colors[category],  # Single color per category\n",
    "                ),\n",
    "                text=category_df.apply(lambda row: f\"Time: {row['ValidTime']:%Y-%m-%d %H:%M UTC}<br>\"\n",
    "                                               f\"Wind Speed (mph): {row['MaxWindSpeed_mph']:.1f}<br>\"\n",
    "                                               f\"Central Pressure (mb): {row['MinPressure']}<br>\"\n",
    "                                               f\"Category: {row['Category']}<br>\"\n",
    "                                               f\"Model: {row['ModelName']}\", axis=1),\n",
    "                hoverinfo='text'\n",
    "            ))\n",
    "\n",
    "    # Set up the map layout with a dark style\n",
    "    fig.update_layout(\n",
    "        mapbox_style='carto-darkmatter',  # Use a dark basemap\n",
    "        mapbox_zoom=4,\n",
    "        mapbox_center={\"lat\": df['Latitude'].mean(), \"lon\": df['Longitude'].mean()},\n",
    "        margin={\"r\": 0, \"t\": 0, \"l\": 0, \"b\": 0},\n",
    "        legend_title_text='Storm Category',\n",
    "        legend=dict(\n",
    "            itemsizing='constant'\n",
    "        )\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "# Helper function to parse latitude and longitude\n",
    "def parse_lat_lon(value):\n",
    "    \"\"\"\n",
    "    Parses latitude or longitude value from ATCF format to decimal degrees.\n",
    "    \"\"\"\n",
    "    if isinstance(value, str):\n",
    "        value = value.strip()\n",
    "        if value and value[-1] in ['N', 'S', 'E', 'W']:\n",
    "            direction = value[-1]\n",
    "            try:\n",
    "                degrees = float(value[:-1]) / 10.0  # Assuming value is in tenths of degrees\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing degrees from value: {value}\")\n",
    "                return None\n",
    "            if direction in ['S', 'W']:\n",
    "                degrees = -degrees\n",
    "            return degrees\n",
    "        elif value:\n",
    "            # If no direction indicator, try to convert directly\n",
    "            try:\n",
    "                return float(value) / 10.0\n",
    "            except ValueError:\n",
    "                print(f\"Error parsing lat/lon value '{value}': cannot convert to float.\")\n",
    "                return None\n",
    "    return None\n",
    "\n",
    "# Helper function to categorize wind speed\n",
    "def wind_speed_to_category(wind_speed):\n",
    "    \"\"\"\n",
    "    Converts wind speed in knots to hurricane category.\n",
    "    \"\"\"\n",
    "    if wind_speed < 34:\n",
    "        return 'Tropical Depression'\n",
    "    elif 34 <= wind_speed <= 63:\n",
    "        return 'Tropical Storm'\n",
    "    elif 64 <= wind_speed <= 82:\n",
    "        return 'Category 1'\n",
    "    elif 83 <= wind_speed <= 95:\n",
    "        return 'Category 2'\n",
    "    elif 96 <= wind_speed <= 112:\n",
    "        return 'Category 3'\n",
    "    elif 113 <= wind_speed <= 136:\n",
    "        return 'Category 4'\n",
    "    elif wind_speed >= 137:\n",
    "        return 'Category 5'\n",
    "    else:\n",
    "        return 'Unknown'\n",
    "\n",
    "# Helper function to map categories to colors\n",
    "def category_to_color(category):\n",
    "    \"\"\"\n",
    "    Maps hurricane category to a color.\n",
    "    \"\"\"\n",
    "    category_colors = {\n",
    "        'Tropical Depression': 'green',\n",
    "        'Tropical Storm': 'blue',\n",
    "        'Category 1': 'yellow',\n",
    "        'Category 2': 'orange',\n",
    "        'Category 3': 'red',\n",
    "        'Category 4': 'orange',\n",
    "        'Category 5': 'magenta',\n",
    "        'Unknown': 'gray'\n",
    "    }\n",
    "    return category_colors.get(category, 'gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed 34 rows with Latitude or Longitude equal to 0.\n"
     ]
    }
   ],
   "source": [
    "fig=create_forecast_map(\"z:\\\\Event_Monitor\\\\forecast_data\\\\aal142024.dat\",\"2024100718\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import tempfile\n",
    "import win32com.client as win32\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "def send_map_via_email(fig, recipients, subject, body, cc=None, bcc=None, sender=None, send_as=False):\n",
    "    \"\"\"\n",
    "    Sends a Plotly map as an HTML attachment via Outlook email.\n",
    "\n",
    "    Parameters:\n",
    "    - fig (plotly.graph_objects.Figure): The Plotly figure to send.\n",
    "    - recipients (list or str): List of recipient email addresses or a single email address.\n",
    "    - subject (str): Subject of the email.\n",
    "    - body (str): Body content of the email. Supports HTML.\n",
    "    - cc (list or str, optional): List of CC email addresses or a single email address.\n",
    "    - bcc (list or str, optional): List of BCC email addresses or a single email address.\n",
    "    - sender (str, optional): Email address to send on behalf of. Must have permissions.\n",
    "    - send_as (bool, optional): If True, attempts to set the 'From' property instead of 'SentOnBehalfOfName'.\n",
    "\n",
    "    Returns:\n",
    "    - None\n",
    "\n",
    "    Raises:\n",
    "    - ValueError: If no valid recipients are provided.\n",
    "    - Exception: If Outlook is not installed or an error occurs during email sending.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Function to validate email addresses using a simple regex\n",
    "        def is_valid_email(email):\n",
    "            regex = r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$'\n",
    "            return re.match(regex, email) is not None\n",
    "\n",
    "        # Helper function to ensure input is a list\n",
    "        def ensure_list(input_field):\n",
    "            if input_field is None:\n",
    "                return []\n",
    "            if isinstance(input_field, str):\n",
    "                input_field = input_field.strip()\n",
    "                return [input_field] if input_field else []\n",
    "            if isinstance(input_field, list):\n",
    "                # Remove any empty strings and strip whitespace\n",
    "                return [email.strip() for email in input_field if email.strip()]\n",
    "            raise ValueError(\"Email fields must be either a string or a list of strings.\")\n",
    "\n",
    "        # Convert recipients, cc, bcc to lists\n",
    "        recipients = ensure_list(recipients)\n",
    "        cc = ensure_list(cc)\n",
    "        bcc = ensure_list(bcc)\n",
    "\n",
    "        # Combine all recipients to ensure at least one is present\n",
    "        all_recipients = recipients + cc + bcc\n",
    "\n",
    "        if not all_recipients:\n",
    "            raise ValueError(\"At least one recipient must be provided in To, Cc, or Bcc.\")\n",
    "\n",
    "        # Validate all email addresses\n",
    "        invalid_emails = [email for email in all_recipients if not is_valid_email(email)]\n",
    "        if invalid_emails:\n",
    "            raise ValueError(f\"The following email addresses are invalid: {invalid_emails}\")\n",
    "\n",
    "        # Create a temporary directory to store the HTML file\n",
    "        with tempfile.TemporaryDirectory() as tmpdirname:\n",
    "            # Define the HTML file path\n",
    "            html_file_path = os.path.join(tmpdirname, 'forecast_map.html')\n",
    "            \n",
    "            # Save the Plotly figure as an HTML file\n",
    "            fig.write_html(html_file_path, full_html=True)\n",
    "            \n",
    "            # Initialize Outlook application\n",
    "            outlook = win32.Dispatch('outlook.application')\n",
    "            mail = outlook.CreateItem(0)  # 0: olMailItem\n",
    "            \n",
    "            # Set email parameters\n",
    "            mail.Subject = subject\n",
    "            mail.Body = body  # Plain text body\n",
    "            mail.HTMLBody = body  # HTML body\n",
    "            \n",
    "            # Set sender if specified\n",
    "            if sender:\n",
    "                try:\n",
    "                    if send_as:\n",
    "                        mail.From = sender  # Requires 'Send As' permissions\n",
    "                    else:\n",
    "                        mail.SentOnBehalfOfName = sender  # Requires 'Send on Behalf Of' permissions\n",
    "                except Exception as e:\n",
    "                    print(f\"Error setting sender to '{sender}': {e}\")\n",
    "                    raise\n",
    "\n",
    "            # Add recipients to To\n",
    "            for recipient in recipients:\n",
    "                mail.Recipients.Add(recipient)\n",
    "            \n",
    "            # Add CC recipients\n",
    "            for c in cc:\n",
    "                mail.Recipients.Add(c)\n",
    "            \n",
    "            # Add BCC recipients\n",
    "            for b in bcc:\n",
    "                mail.Recipients.Add(b)\n",
    "            \n",
    "            # Resolve all recipients\n",
    "            if not mail.Recipients.ResolveAll():\n",
    "                unresolved = [rec.Name for rec in mail.Recipients if not rec.Resolved]\n",
    "                raise ValueError(f\"Some recipients could not be resolved: {unresolved}\")\n",
    "            \n",
    "            # Attach the HTML file\n",
    "            mail.Attachments.Add(Source=html_file_path)\n",
    "            \n",
    "            # Send the email\n",
    "            mail.Send()\n",
    "            \n",
    "            print(\"Email sent successfully.\")\n",
    "    \n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError: {ve}\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while sending the email: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Email sent successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Update the layout to include mapbox settings\n",
    "fig.update_layout(\n",
    "    mapbox=dict(\n",
    "        accesstoken='pk.eyJ1IjoiYWx2YXJvZmFyaWFzIiwiYSI6ImNtMXptbm9iaDA4OHMybG9vc3VqdW1vZ3oifQ.ZJ8d6gNAiR1htIYxESOYuQ',  # Replace with your Mapbox token\n",
    "        style='carto-darkmatter',\n",
    "        center=dict(lat=0, lon=0),\n",
    "        zoom=1\n",
    "    ),\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "# Define email parameters\n",
    "recipients = ['swyatt11@gmail.com','alvaro.farias@lockton.com']\n",
    "\n",
    "subject = 'Forecast Map'\n",
    "body = \"\"\"\n",
    "<html>\n",
    "<head></head>\n",
    "<body>\n",
    "    <p>Dear Team,</p>\n",
    "    <p>Please find the attached forecast map.</p>\n",
    "    <p>Best regards,<br>Lockton Storm Monitor</p>\n",
    "</body>\n",
    "</html>\n",
    "\"\"\"\n",
    "\n",
    "# Optional parameters\n",
    "cc = ['alvaro.farias.velasco@gmail.com']\n",
    "bcc = ['']\n",
    "sender = 'alvaro.farias@lockton.com'  # Optional: specify if needed\n",
    "\n",
    "# Send the email\n",
    "send_map_via_email(fig, recipients, subject, body, cc=cc, bcc=bcc, sender=sender)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EventMonitor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
